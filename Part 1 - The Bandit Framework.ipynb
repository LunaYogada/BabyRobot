{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Baby Robot's Guide To Reinforcement Learning\n",
    "\n",
    "## Part 1: The Bandit Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Photo by Ferenc Almasi on Unsplash](https://cdn-images-1.medium.com/max/800/0*WUKevSQJrdR5Xjwr)\n",
    "\n",
    "Photo by __[Ferenc Almasi](https://medium.com/r/?url=https%3A%2F%2Funsplash.com%2F%40flowforfrank%3Futm_source%3Dmedium%26utm_medium%3Dreferral)__ on __[Unsplash](https://medium.com/r/?url=https%3A%2F%2Funsplash.com%3Futm_source%3Dmedium%26utm_medium%3Dreferral)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "\n",
    "* Baby Robot is lost. Using Reinforcement Learning we need to help him to find his way back to his mum.¬†\n",
    "\n",
    "* Here we cover the \"Bandit Framework\", the first step on the road to Reinforcement Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Once upon a time there was a Baby Robot. One day he was out shopping with his mum when a robot dog ran past. Baby Robot's mum was busy buying some new spark plugs, so didn't notice when he turned around and started to follow the dog.\n",
    "\n",
    "He chased the dog through the crowded shopping mall, along several aisles, down escalators and up escalators until, finally, the dog ducked into a service hatch in the wall and was gone. Not only was the service hatch too small for Baby Robot to fit through, but he also realised that he was lost. Well and truly lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Power Socket¬†Problem\n",
    "\n",
    "To make matters worse Baby Robot realised that he was nearly out of power. If he was ever going to find his mum again he'd need to recharge. And quickly.\n",
    "\n",
    "Luckily, there was a charging station directly opposite the point where he'd lost sight of the dog. He made his way over and found that the room contained 5 separate power outlets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kernel & front-end diagram](Images/power_socket.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/power_socket.png\"/>\n",
    "\n",
    "![](Images/power_socket.png)\n",
    "\n",
    "![Kernel & front-end diagram](Images/power_socket.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"baby_robot_2.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He moved to the first power point and plugged in. Disappointingly, he only received a burst of power that would keep him running for 2 seconds. He plugged into the same power point again and this time received 3 seconds worth of charge. Better, but still not great. At this rate it would take him forever to fully recharge. Maybe one of the other power points would be better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/baby_robot_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in mind, he moved to the next power point. Another 3 seconds of charge. This outlet didn't seem any better than the first, so he moved to try the third socket. This time he received 4 seconds worth. This appeared to be the power outlet that would give him the most charge and, while it would still take a long time to fully recharge, it seemed better than the other sockets he'd tried. So, resolving to stay at this power point, he plugged in again. This time he received a measly 2 seconds worth of charge.\n",
    "\n",
    "Rather than just moving along the sockets, he was going to have to find a better way to discover which was the best and would let him get to maximum charge the fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Multi-Armed Bandit Problem\n",
    "\n",
    "\n",
    "*This power outlet problem is analogous to the standard, multi-armed, bandit problem used to illustrate how exploration can be examined in isolation.*\n",
    "\n",
    "*In the multi-armed bandit you are trying to win as much money as possible from playing a set of one-armed bandits (otherwise known as slot machines or fruit machines), each of which can give a different payout. You need to find which machine gives the biggest payout, so you can make as much money as possible in the allocated time.*\n",
    "\n",
    "*Each play of a machine (or pull of the bandit's arm) corresponds to one time slot and you only get to play for a fixed number of time slots.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Exploration-Exploitation Dilemma\n",
    "\n",
    "Baby Robot is faced with the problem of not knowing which power outlet will give him the most charge. He therefore needs to explore the possible choices in search of the best one.\n",
    "\n",
    "However, because he's short on time, he can't take too long to make up his mind as to which is the best, nor can he test each power socket to know exactly how much charge it will give on average. He needs to exploit the knowledge he gains, so that he doesn't waste time trying bad power outlets, to allow him to get the maximum amount of power in the shortest possible time.\n",
    "\n",
    "This is an example of the classic exploration-exploitation dilemma, in which you want to explore the possible options in search of the best one while, at the same time, wanting to exploit the information that has already been obtained, so that you can gain the maximum possible overall reward.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Mathematical Framework and Terminology\n",
    "\n",
    "\n",
    "To help Baby Robot charge up, and get him back looking for his mum, we'll first need to familiarise ourselves with some of the common terms and mathematical notation used in Reinforcement Learning.¬†\n",
    "\n",
    "The notation used largely follows that of Sutton & Barto, from their Bible of Reinforcement Learning, __[\"Reinforcement Learning: An Introduction\"](https://medium.com/r/?url=https%3A%2F%2Famzn.to%2F2RIZ9pc)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Action***\n",
    "\n",
    "In Reinforcement Learning making a choice between the available options, and selecting that option, is known as taking an action. For example, in the power point problem, an action would be selecting one of the available power sockets. In the multi-armed bandit problem it would be choosing and playing one of the set of slot machines.\n",
    "\n",
    "In the simple power socket and bandit problems actions are taken at discrete time steps. In other words, one action is taken after another and, in these problems, there are a fixed number of total actions.¬†\n",
    "\n",
    "* the action taken at time-step 't' is denoted as 'A‚Çú'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Reward***\n",
    "\n",
    "Additionally, every time an action is taken a reward is obtained. In the power socket problem the reward is an amount of charge; whereas, in the bandit problem, it is an amount of money, won from the machine.¬†\n",
    "\n",
    "The reward obtained for taking a particular action is a random value, drawn from an underlying probability distribution, specific to that action. So each time an action is taken the reward returned can have a different value. If the same action is repeated multiple times, then a more accurate estimate of the true mean of the reward can be calculated.\n",
    "\n",
    "Obtaining a reward for taking a particular action is probably the main defining feature of Reinforcement Learning. It is this reward that is used to guide learning, trying to find the best actions and thereby maximise the total overall reward. This is in contrast to supervised learning, in which the best actions would be provided as part of the training data.\n",
    "\n",
    "* the reward obtained at time-step 't', after taking action 'A‚Çú', is denoted as 'R‚Çú'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Number of Actions***\n",
    "\n",
    "The number of available actions is denoted by the letter 'k'. So, in the power socket example, 'k' would be the number of power sockets to choose from. Since, in this case, there are 5 different sockets, 'k' would be 5. In the multi-armed bandit problem, it is the total number of slot-machines to choose from (indeed this problem is often referred to as the 'k-armed bandit problem').\n",
    "\n",
    "*the number of available actions is denoted by the letter 'k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Expected Reward***\n",
    "\n",
    "Each of the available 'k' actions has an ***expected*** reward, where the term 'expected' refers to the mean value that would be obtained if that action was repeated multiple times. So, for example, if a fair coin was being tossed, the expected probability of obtaining heads would be 0.5 since, on average, heads should appear for half of all coin tosses when the total number of tosses is large.\n",
    "\n",
    "* the expected value of a property is denoted by the symbol 'ùîº'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Value***\n",
    "\n",
    "The expected reward of an action is known as the value of that action and is denoted as 'q(a)' where 'a' is the specific action chose at time step 't' (i.e. A‚Çú = a).\n",
    "\n",
    "* So the value of an action 'a' is given by:\n",
    "\n",
    "\n",
    "<img src=\"Images/formula_1.png\">\n",
    "\n",
    "\n",
    "This can be read as: *the value of action 'a' is equal to the expected (mean) value of the reward, given that the action chosen at time-step 't' is action 'a'.*\n",
    "\n",
    "(NB: any time you see the symbol '|' in a probability equation, just read it as 'given that').\n",
    "\n",
    "Effectively this says that if you keep repeating action 'a' and average the reward values you get back then, ultimately, you'll end up knowing the true value 'q(a)', the mean reward for 'a'. So, if Baby Robot keeps trying the same power socket he'll get an increasingly accurate estimate of that socket's true mean power output until, eventually, if he repeats the action for long enough, he'll know the true value of that socket's output.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sample Average Estimates\n",
    "\n",
    "Obviously we don't know the true value of the actions' rewards. If we did then things would be simple. We'd just exploit the knowledge we have without having to do any exploration. We'd just play the best slot machine to win the most money and Baby Robot would plug into the socket that gave the most charge.¬†\n",
    "\n",
    "However, this isn't the case. We don't know the true rewards and therefore must do some exploration to find how the rewards compare, from each of the possible actions. Consequently, to keep track of which action is best, as we explore the set of possible actions, we need to make an estimate of each action's value. As time progresses, this estimate should get progressively more accurate and converge upon the true reward value.\n",
    "\n",
    "Since the true value of an action is the mean reward for that action, a simple, but effective, estimate can be calculated by taking the average value of the rewards returned so far for that action.\n",
    "\n",
    "So, 'Q‚Çú(a)', the estimated value of action 'a' at time step 't', is given by:\n",
    "\n",
    "\n",
    "<img src=\"Images/formula_2.png\">\n",
    "\n",
    "\n",
    "where 'n' is the number of times that action 'a' was taken, prior to time 't', and 'R·µ¢' is the reward obtained at each of the time steps when action 'a' was taken."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Calculating the Sample¬†Average\n",
    "\n",
    "The simplest way to form the sum of all rewards, for any action, is to store each of the rewards and then add them when required. However, from a practical point of view this isn't very efficient, both in terms of storage and computing time. It would be possible to keep track of the summed rewards, but even this value would grow to be unmanageable over time.\n",
    "\n",
    "* A better solution is to calculate the new estimated reward based on the last estimate.\n",
    "\n",
    "For an action 'a', the n·µó ∞ estimate for the action-value, 'Q‚Çô', is given by the sum of all previous rewards obtained for that action, divided by the number of times that action has been selected (i.e. it's just the average value):\n",
    "\n",
    "\n",
    "<img src=\"Images/formula_3.png\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, swapping things around, the sum of the rewards, prior to 'n', is given by:\n",
    "\n",
    "\n",
    "<img src=\"Images/formula_4.png\">\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the next reward, 'R‚Çô', is obtained, the new estimate can be calculated by adding this to the previous sum of the rewards and increasing the count of the number of times the action has been taken. So the new estimate is:\n",
    "\n",
    "\n",
    "<img src=\"Images/formula_5.png\">\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/formula_6.png\">\n",
    "\n",
    "\n",
    "Since we already know how to write the sum of the rewards, prior to 'n', in terms of the last estimate, we can simply swap this into the equation:\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/formula_7.png\">\n",
    "\n",
    "\n",
    "Rearranging this, we end up with a usable form for the new estimate, expressed in terms of the last estimate 'Q‚Çô' and the new reward 'R‚Çô':\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/formula_8.png\">\n",
    "\n",
    "<i><center>Formula 1: The new estimate, calculated in terms of the last estimate and the new reward.</center></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing Selection Strategies\n",
    "\n",
    "Now we have all the required notation and terminology in place, we can build a test system. This will let us examine how different strategies perform at finding and exploiting the best power socket. Remember that we want to get Baby Robot charged up in the minimum amount of time, so we need to locate the best socket and then use it until charging is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Power¬†Socket\n",
    "\n",
    "A python implementation of a power socket is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerSocket:\n",
    "    \"\"\" the base power socket class \"\"\"\n",
    "    \n",
    "    def __init__(self, q):                \n",
    "        self.q = q        # the true reward value              \n",
    "        self.initialize() # reset the socket\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.Q = 0   # the estimate of this socket's reward value                \n",
    "        self.n = 0   # the number of times this socket has been tried        \n",
    "    \n",
    "    def charge(self):\n",
    "        \"\"\" return a random amount of charge \"\"\"\n",
    "        \n",
    "        # the reward is a guassian distribution with unit variance around the true\n",
    "        # value 'q'\n",
    "        value = np.random.randn() + self.q        \n",
    "        \n",
    "        # never allow a charge less than 0 to be returned        \n",
    "        return 0 if value < 0 else value\n",
    "                    \n",
    "    def update(self,R):\n",
    "        \"\"\" update this socket after it has returned reward value 'R' \"\"\"     \n",
    "    \n",
    "        # increment the number of times this socket has been tried\n",
    "        self.n += 1\n",
    "\n",
    "        # the new estimate of the mean is calculated from the old estimate\n",
    "        self.Q = (1 - 1.0/self.n) * self.Q + (1.0/self.n) * R\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\" return an estimate of the socket's reward value \"\"\"\n",
    "        return self.Q"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the '__init__' (constructor) function, we just initialise the variables used by the socket. These are:\n",
    "\n",
    "q = the true mean value of the socket output¬†\n",
    "Q = the running estimate of the socket output (i.e. its reward)\n",
    "n = the number of times the socket has been tried\n",
    "\n",
    "When estimating the socket output, the true output 'q' is the value that we're trying to converge upon. Although 'q' is required to setup the socket, it's value isn't used elsewhere, since this is the value we're trying to find.\n",
    "\n",
    "The 'charge' function returns the output reward for the socket. This is given by a normal (Gaussian) distribution, with a mean value of 'q' (the value provided during setup).\n",
    "\n",
    "The 'update' function calculates the sample average estimate, using the new reward value and the previous estimate, as described in formula 1 in the previous section. In code this translates directly to be:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Charging¬†Room\n",
    "\n",
    "We want to setup a room that has 5 sockets, each of which will return a burst of power with a mean duration that is different to the other sockets.\n",
    "\n",
    "So, in the code below, we create the 5 sockets and randomly assign them a mean value that is given by a random value between 1 and 5, doubled and offset by 2. This creates distinct power outputs and keeps the outputs above zero. Note that the room is created randomly each time this code is run, so that the socket which is best will potentially change each time (and therefore won't match with the outputs that Baby Robot was seeing!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.core.pylabtools import figsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 5 sockets in a fixed order\n",
    "socket_order = [2,1,3,5,4]\n",
    "\n",
    "# create the sockets\n",
    "# - the mean value of each socket is derived from the socket order index, which is doubled to give \n",
    "#   distinct values and offset by 2 to keep the distribution above zero\n",
    "sockets = [PowerSocket((q*2)+2) for q in socket_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_SOCKETS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a8d8bda453ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# rewards will contain the charge returned at all of the time steps for each socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTOTAL_STEPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNUM_SOCKETS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# iterate through each of the sockets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NUM_SOCKETS' is not defined"
     ]
    }
   ],
   "source": [
    "# using a very large number of time steps just to create sufficient data to get smooth plots of socket output\n",
    "TOTAL_STEPS = 100000\n",
    "\n",
    "# rewards will contain the charge returned at all of the time steps for each socket \n",
    "rewards = np.zeros(shape=(TOTAL_STEPS,NUM_SOCKETS))\n",
    "\n",
    "# iterate through each of the sockets\n",
    "for socket_number,socket in enumerate(sockets):\n",
    "    \n",
    "    # get charge from the socket for the defined number of steps   \n",
    "    for t in range(TOTAL_STEPS): rewards[t,socket_number] = socket.charge()                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code we test each socket for a whopping 100,000 time steps. This is only to get smooth curves for our graphs, to show exactly how the output distribution varies for each socket.¬†\n",
    "Running this, we get the following type of output from each socket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the charge we got from the sockets           \n",
    "plt.violinplot(rewards)   \n",
    "plt.xlabel('Socket')\n",
    "plt.ylabel('Reward Distribution (seconds of charge)') \n",
    "plt.title('Violin plot of the reward distribution for each socket')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"violin\" plot above, the line for each socket shows the range of values returned as the reward for each socket. So socket number 1 returned between approximately 2 and 10 seconds of charge. The shaded area, underneath the line, represents the frequency at which each reward was returned. For socket 1 the most frequently returned reward was 6 seconds of charge. This is the mean reward for this socket and also its true reward value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the sockets to draw each plot\n",
    "# (from: https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0)\n",
    "for socket_number in range(NUM_SOCKETS):\n",
    "    sns.distplot(pd.DataFrame(rewards[:,socket_number]), hist = False, kde = True,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 2},\n",
    "                 label = f'{socket_number+1}');\n",
    "    \n",
    "# Plot formatting\n",
    "plt.xlim(0, None)\n",
    "plt.legend(title = 'Sockets')\n",
    "plt.title('Density Plot of Socket Outputs')\n",
    "plt.xlabel('Socket Output (seconds of charge)')\n",
    "plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the density plot we can see how the reward probability varies for each socket. In this run of the code, socket number 4 is the best, giving an average of 12 seconds worth of charge, whereas socket 2 is the worst, giving only 4 seconds worth.¬†\n",
    "\n",
    "To keep the examples consistent, we'll continue to use these reward probabilities for the remainder of the examples, so socket 4 is the optimal socket that we're trying to find and exploit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
